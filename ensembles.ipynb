{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#B31B1'> Ensemble Methods </font>\n",
    "\n",
    "So far we've seen how to construct a single decision tree, now we'll see how to combine multiple trees together into a more powerful ensemble method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(6,6)}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B31B1'> California Housing Dataset </font>\n",
    "\n",
    "We'll use the boston housing dataset, the goal of which is to predict house prices in California from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "data = fetch_california_housing()\n",
    "X = data['data']\n",
    "Y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude      Y  \n",
       "0    -122.23  4.526  \n",
       "1    -122.22  3.585  \n",
       "2    -122.24  3.521  \n",
       "3    -122.25  3.413  \n",
       "4    -122.25  3.422  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = (pd.DataFrame(X, columns = data['feature_names'])\n",
    "           .assign(Y = Y))\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B31B1'> Bagging </font>\n",
    "Bagging is the process of generating a set of weak learners by training on random bootstrapped samples of our dataset (i.e. sampling a dataset from our training data with replacement). To show the power of bagging, we can use random trees (i.e. trees generated by randomnly generating splits in each node, then using the most common value in each leaf node as our prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Random trees are usually used just in ensemble methods, so we have to manually specify we only want one to start\n",
    "random_tree = ExtraTreesRegressor(n_estimators = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `ExtraTreesClassifier` not found.\n"
     ]
    }
   ],
   "source": [
    "ExtraTreesClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that on its own, the random tree has a mean squared error of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9910140910747239"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(random_tree, X, Y,\n",
    "                scoring=\"neg_mean_squared_error\", \n",
    "                cv=3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could perform bagging by randomnly generating the bootstrap samples ourself.... or we can use scikit-learn's BaggingRegressor or BaggingClassifier! We simply need to specify the number of weak learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagged_random_trees = BaggingRegressor(base_estimator = ExtraTreesRegressor(n_estimators = 1),\n",
    "                                        n_estimators = 10\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging the random trees together leads to a big jump in performance... even though they're random trees!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4593573009739824"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(bagged_random_trees, X, Y,\n",
    "                scoring=\"neg_mean_squared_error\", \n",
    "                cv=3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also play around with how the performance changes as alter the number of estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.41984715908458403"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagged_random_trees = BaggingRegressor(base_estimator = ExtraTreesRegressor(n_estimators = 1),\n",
    "                                        n_estimators = 100\n",
    "                                       )\n",
    "cross_val_score(bagged_random_trees, X, Y,\n",
    "                scoring=\"neg_mean_squared_error\", \n",
    "                cv=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4162926960466282"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagged_random_trees = BaggingRegressor(base_estimator = ExtraTreesRegressor(n_estimators = 1),\n",
    "                                        n_estimators = 200\n",
    "                                       )\n",
    "cross_val_score(bagged_random_trees, X, Y,\n",
    "                scoring=\"neg_mean_squared_error\", \n",
    "                cv=3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here changing the number from 10 to 100 estimators has a large increase in performance, but adding another 100 after doesn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B31B1'> Random Forests </font>\n",
    "\n",
    "Random forests is a bagging approach for trees that also randomnly selects the set of features each tree can use (to help decorrelate results). We can access a great implementation of random forests using scikit-learn.\n",
    "\n",
    "In addition to all the same decision tree hyperparameters to change, random forests also let us choose the number of trees to make, whether or not we use bootstrapped samples for each tree, and the max number of features every tree can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.45750622317994366"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "random_forest = RandomForestRegressor(n_estimators = 100)\n",
    "\n",
    "cross_val_score(random_forest, X, Y,\n",
    "                scoring=\"neg_mean_squared_error\", \n",
    "                cv=3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B31B1'> Gradient Boosting </font>\n",
    "Recall that boosting is the process of sequentially training weak learners to create a powerful prediction. In gradient boosting, each subsequent model is going to try to replicate the gradient of the loss function evaluated at the current model (almost mimicing gradient descent!). Let's try walking through a simple example manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start by splitting our data into training and testing\n",
    "train_df = data_df.sample(frac=0.8)\n",
    "test_df = data_df[~data_df.index.isin(train_df.index)]\n",
    "\n",
    "X_tr = train_df.drop('Y',axis=1)\n",
    "Y_tr = train_df['Y']\n",
    "\n",
    "X_tst = test_df.drop('Y',axis=1)\n",
    "Y_tst = test_df['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating our initial predictions, in this case be just fitting a decision tree to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our initial training MSE is  0.4805573619466173\n"
     ]
    }
   ],
   "source": [
    "# Start with our base prediction using a decision tree with only 5 layers\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "base_tree = DecisionTreeRegressor(max_depth=5)\n",
    "\n",
    "base_tree.fit(X_tr, Y_tr)\n",
    "\n",
    "#Current MSE\n",
    "print('Our initial training MSE is ', np.mean((base_tree.predict(X_tr) - Y_tr)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to compute the gradient so we can construct a training dataset for our second tree. Since our objective is mean squared error, our gradient is going to be $\\hat{y} - y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residuals =  base_tree.predict(X_tr) - Y_tr\n",
    "\n",
    "second_tree = DecisionTreeRegressor(max_depth=5)\n",
    "second_tree.fit(X_tr, residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we figure out the step size using line search (we'll just manually try gamma values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best step size was  1.0  for a new MSE of  0.38376731258244595\n"
     ]
    }
   ],
   "source": [
    "best_mse = 99999\n",
    "best_gamma = None\n",
    "\n",
    "for gamma in np.linspace(0.01, 1, 100):\n",
    "    mse =  np.mean((base_tree.predict(X_tr) - gamma*second_tree.predict(X_tr) - Y_tr)**2)\n",
    "    if mse < best_mse:\n",
    "        best_gamma = gamma\n",
    "        best_mse = mse\n",
    "\n",
    "print('The best step size was ',best_gamma,' for a new MSE of ', best_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now continue this process and try to add in a third tree and so on... Instead let's show how to do this with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#For gradient boosted trees we can pick our loss function, a fixed learning rate, \n",
    "# and fiddle with all the usual decision tree hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient boosted MSE is  0.26315018593495404\n"
     ]
    }
   ],
   "source": [
    "grad_boost_tree = GradientBoostingRegressor(loss = 'ls')\n",
    "\n",
    "grad_boost_tree.fit(X_tr, Y_tr)\n",
    "\n",
    "print('The gradient boosted MSE is ', np.mean((grad_boost_tree.predict(X_tr) - Y_tr)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare the test set error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original tree MSE is  0.4940463785109224\n",
      "The one-step boosted tree MSE is  0.412560816891238\n",
      "The gradient boosted test MSE is  0.27623892788197096\n"
     ]
    }
   ],
   "source": [
    "print('The original tree MSE is ', np.mean((base_tree.predict(X_tst) - Y_tst)**2))\n",
    "print('The one-step boosted tree MSE is ', np.mean((base_tree.predict(X_tst) + best_gamma*second_tree.predict(X_tst) - Y_tst)**2))\n",
    "print('The gradient boosted test MSE is ', np.mean((grad_boost_tree.predict(X_tst) - Y_tst)**2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
