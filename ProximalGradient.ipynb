{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Plots.PyPlotBackend()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots, Random, LinearAlgebra, Statistics, SparseArrays\n",
    "pyplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prox of nonnegative regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prox of nonneg evaluated at z minimizes obj(x) = I(x>=0) + 1/2(x-z)^2\n",
    "\n",
    "z = -2 # try out different values!\n",
    "x = -5:.1:5\n",
    "objx = [xi<0 ? Inf : 0 + 1/2*(xi-z)^2 for xi in x]\n",
    "plot(x, objx)\n",
    "xlabel!(\"x\")\n",
    "ylabel!(\"obj(x)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prox_nonneg(x) = max.(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = randn(2)\n",
    "x /= norm(x)\n",
    "px = prox_nonneg(x)\n",
    "\n",
    "plot([x[1]],[x[2]],shape=:o,label=\"x\")\n",
    "plot!([px[1]],[px[2]],shape=:d,label=\"prox_nonnneg(x)\")\n",
    "xlims!((-2,2))\n",
    "ylims!((-2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prox of l1 regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prox of l1 evaluated at z minimizes |x| + 1/2(x-z)^2\n",
    "z = -3\n",
    "x = -3:.1:3\n",
    "objx = [abs(xi) + 1/2*(xi-z)^2 for xi in x]\n",
    "plot(x, objx)\n",
    "xlabel!(\"x\")\n",
    "ylabel!(\"obj(x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function prox_l1(x::Number, alpha=1)\n",
    "    if x > alpha\n",
    "        return x-alpha\n",
    "    elseif x < -alpha\n",
    "        return x + alpha\n",
    "    else\n",
    "        return 0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show x = randn()\n",
    "@show px = prox_l1(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = -2:.1:2\n",
    "px = [prox_l1(xi) for xi in x]\n",
    "plot(x, px)\n",
    "xlabel!(\"x\")\n",
    "ylabel!(\"prox_l1(x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show x = randn(2)\n",
    "@show px = prox_l1.(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# proximal gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla \\|Xw-y\\|^2 = \\nabla((Xw-y)^T (Xw-y)) = 2X^T(Xw-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proximal gradient method for quadratic loss and nonnegative regularizer\n",
    "function proxgrad_quad_nonneg(X, y; maxiters = 10, stepsize = 1, w = zeros(size(X,2)))\n",
    "    w = zeros(size(X,2))\n",
    "    objval = Float64[]\n",
    "    for i=1:maxiters\n",
    "        # gradient step\n",
    "        g = 2X'*(X*w-y) # compute quadratic gradient\n",
    "        z = w - stepsize*g\n",
    "        # prox step\n",
    "        w = prox_nonneg(z)\n",
    "        # record objective value\n",
    "        push!(objval, norm(X*w-y)^2 + any(w<0) ? Inf : 0)\n",
    "    end\n",
    "    return w, objval\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proximal gradient method for quadratic loss and l1 regularizer\n",
    "function proxgrad_quad_l1(X, y; maxiters = 10, stepsize = 1., λ = 1., w = zeros(size(X,2)))\n",
    "    objval = Float64[]\n",
    "    for i=1:maxiters\n",
    "        # gradient step\n",
    "        g = 2X'*(X*w-y) # compute quadratic gradient\n",
    "        z = w - stepsize*g\n",
    "        # prox step\n",
    "        myprox(z) = prox_l1(z, stepsize*λ)\n",
    "        w = myprox.(z)\n",
    "        # record objective value\n",
    "        push!(objval, norm(X*w-y)^2 + norm(w,1))\n",
    "    end\n",
    "    return w, objval\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's solve the problem $$\\text{minimize} \\quad (y-w)^2 + |w|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2\n",
    "wvec = -3:.1:3\n",
    "objwvec = [(wi-y)^2 + abs(wi) for wi in wvec]\n",
    "plot(wvec, objwvec, label=\"objective\")\n",
    "xlabel!(\"w\")\n",
    "ylabel!(\"obj(w)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [-2]\n",
    "X, y = ones(1,1), 2*ones(1)\n",
    "w, obj = proxgrad_quad_l1(X, y; maxiters = 10, stepsize = .2, w = w)\n",
    "\n",
    "plot(wvec, objwvec, label=\"objective\")\n",
    "xlabel!(\"w\")\n",
    "ylabel!(\"obj(w)\")\n",
    "plot!([w], [obj[end]], color=:red, shape=:o, label=\"final iterate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduce LowRankModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LowRankModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss = QuadLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularizers\n",
    "lambda = 1\n",
    "\n",
    "nonneg = NonNegConstraint()\n",
    "l1 = OneReg(lambda)\n",
    "l2 = QuadReg(lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the quad loss returns the sum of square differences between its first and second argument\n",
    "evaluate(loss, 2., 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(3*loss, 2., 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(1/2*loss, [2., 2.], [3., 3.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also evaluate the gradient wrt the first argument\n",
    "grad(loss, 2., 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad(loss, 0., 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad(L1Loss(), 2., 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can evaluate the proximal operator of the regularizer\n",
    "prox(nonneg, [-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can evaluate the proximal operator of lambda times the regularizer\n",
    "λ = .01\n",
    "prox(l1, 1, λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain rule: \n",
    "# gradient of ||Xw - y||^2 wrt w is X' * <gradient of ||z-y||^2 wrt z>, \n",
    "# where z = X*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import LowRankModels: evaluate, grad\n",
    "evaluate(loss::Loss, X::Array{Float64,2}, w, y) = evaluate(loss, X*w, y)\n",
    "grad(loss::Loss, X::Array{Float64,2}, w, y) = X'*grad(loss, X*w, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proximal gradient method\n",
    "function proxgrad(loss, reg, X, y; maxiters = 10, stepsize = 1)\n",
    "    w = zeros(size(X,2))\n",
    "    objval = Float64[]\n",
    "    for i=1:maxiters\n",
    "        # gradient step\n",
    "        g = grad(loss, X, w, y)\n",
    "        z = w - stepsize*g\n",
    "        # prox step\n",
    "        w = prox(reg, z, stepsize)\n",
    "        # record objective value\n",
    "        push!(objval, evaluate(loss, X, w, y) + evaluate(reg, w))\n",
    "    end\n",
    "    return w, objval\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proximal gradient method\n",
    "function proxgrad(loss::Loss, reg::Regularizer, X, y;\n",
    "                  maxiters::Int = 10, stepsize::Number = 1., \n",
    "                  ch::ConvergenceHistory = ConvergenceHistory(\"proxgrad\"))\n",
    "    w = zeros(size(X,2))\n",
    "    for t=1:maxiters\n",
    "        t0 = time()\n",
    "        # gradient step\n",
    "        g = grad(loss, X, w, y)\n",
    "        w = w - stepsize*g\n",
    "        # prox step\n",
    "        w = prox(reg, w, stepsize)\n",
    "        # record objective value\n",
    "        update_ch!(ch, time() - t0, obj = evaluate(loss, X, w, y) + evaluate(reg, w))\n",
    "    end\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(0)\n",
    "X, y = rand(6,3), rand(6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = ConvergenceHistory(\"NNLS\")\n",
    "w = proxgrad(QuadLoss(), NonNegConstraint(), X, y; \n",
    "             stepsize=.001, maxiters=50,\n",
    "             ch = ch)\n",
    "\n",
    "@show ch.objective\n",
    "plot(ch.objective, label=\"NNLS\")\n",
    "xlabel!(\"iteration\")\n",
    "ylabel!(\"objective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(ch.objective .- ch.objective[end], label=\"NNLS\") # try semilog\n",
    "xlabel!(\"iteration\")\n",
    "ylabel!(\"objective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's generate some more data and test out these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function generate_data(n, w)\n",
    "    X = randn(n,length(w))\n",
    "    y = X*w\n",
    "    return X, y\n",
    "end\n",
    "\n",
    "function generate_noisy_data(n, w)\n",
    "    X = randn(n,length(w))\n",
    "    y = X*w + .1*randn(n)\n",
    "    return X, y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's repeat what we did in the regularized regression notebook, using our nifty proximal gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare different kinds of regularized regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ridge_regression(X,y; λ=1, kwargs...)\n",
    "    w = proxgrad(QuadLoss(), λ*QuadReg(), X, y; kwargs...)\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function nnls(X,y; kwargs...)\n",
    "    w = proxgrad(QuadLoss(), NonNegConstraint(), X, y; kwargs...)\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function lasso(X,y; λ=1, kwargs...)\n",
    "    w = proxgrad(QuadLoss(), λ*OneReg(), X, y; kwargs...)\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "\n",
    "d = 30\n",
    "w_randn = randn(d)\n",
    "w_sparse = sprandn(d, .5)\n",
    "w_pos = sprand(d, .5);\n",
    "\n",
    "w = w_sparse\n",
    "\n",
    "X, y = generate_noisy_data(30, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxiters = 10000\n",
    "stepsize = .1/norm(X)\n",
    "\n",
    "w_ridge = ridge_regression(X,y, maxiters=maxiters, stepsize=stepsize)\n",
    "w_nonneg = nnls(X,y, maxiters=maxiters, stepsize=stepsize)\n",
    "w_lasso = lasso(X,y, maxiters=maxiters, stepsize=stepsize);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(w_ridge, label=\"ridge coefficients\", bins=-3:.1:3, alpha=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(w_lasso, label=\"lasso coefficients\", bins=-3:.1:3, alpha=.7)\n",
    "histogram!(w_ridge, label=\"ridge coefficients\", bins=-3:.1:3, alpha=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histogram(w_nonneg, label=\"nonnegative coefficients\", bins=-3:.1:3, alpha=.7)\n",
    "histogram!(w_ridge, label=\"ridge coefficients\", bins=-3:.1:3, alpha=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which fits data best?\n",
    "Xtest,ytest = generate_data(20,w)\n",
    "\n",
    "scatter(ytest,Xtest*w_ridge,label=\"ridge\")\n",
    "scatter!(ytest,Xtest*w_lasso,label=\"lasso\")\n",
    "scatter!(ytest,Xtest*w_nonneg,label=\"NNLS\")\n",
    "plot!(ytest,ytest,label=\"true model\")\n",
    "xlabel!(\"true value\")\n",
    "ylabel!(\"predicted value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validate over lambda\n",
    "Random.seed!(1)\n",
    "\n",
    "w = randn(40)\n",
    "X,y = generate_noisy_data(30, w)\n",
    "Xtest,ytest = generate_noisy_data(30, w)\n",
    "\n",
    "maxiters = 10000\n",
    "stepsize = .1/norm(X)\n",
    "\n",
    "ridge_error = Float64[]\n",
    "lasso_error = Float64[]\n",
    "λs = 0:.1:2\n",
    "for λ in λs\n",
    "    w = ridge_regression(X,y, λ=λ, maxiters=maxiters, stepsize=stepsize)\n",
    "    push!(ridge_error, sum((ytest - Xtest*w).^2))\n",
    "    w = lasso(X,y, λ=λ, maxiters=maxiters, stepsize=stepsize);\n",
    "    push!(lasso_error, sum((ytest - Xtest*w).^2))\n",
    "end\n",
    "plot(λs, lasso_error, label=\"lasso\")\n",
    "plot!(λs, ridge_error, label=\"ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
