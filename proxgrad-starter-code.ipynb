{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling LowRankModels [15d4e49f-4837-5ea3-a885-5b28bfa376dc]\n",
      "└ @ Base loading.jl:1278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "proxgrad_const"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots, Random, LinearAlgebra, Statistics, SparseArrays\n",
    "include(\"proxgrad.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving ERM problems\n",
    "\n",
    "The file `proxgrad.jl` contains code for solving regularized empirical risk minimization (ERM) problems. It provides the optimization function `proxgrad` together with a large number of predefined loss functions and regularizers.\n",
    "    \n",
    "The function `proxgrad` solves regularized ERM problems of the form\n",
    "$$\n",
    "\\mbox{minimize} \\quad \\sum_{i=1}^n \\ell(y_i, w^T x_i) + r(w).    \n",
    "$$\n",
    "It solves these with the proximal gradient method, which we will learn shortly.\n",
    "\n",
    "You can select from a range of losses. For real valued $y$, try:\n",
    "   * quadratic loss - `QuadLoss()`\n",
    "   * $\\ell_1$ loss - `L1Loss()`\n",
    "   * quantile loss (for $\\alpha$ quantile) - `QuantileLoss(α)`\n",
    " \n",
    "For Boolean $y$, try\n",
    "   * hinge loss - `HingeLoss()`\n",
    "   * logistic loss - `LogisticLoss()`\n",
    "   * weighted hinge loss - `WeightedHingeLoss()`\n",
    "\n",
    "For nominal $y$, try\n",
    "   * multinomial loss - `MultinomialLoss()`\n",
    "   * one vs all loss - `OvALoss()`\n",
    "       * (by default, it uses the logistic loss for the underlying binary classifier)\n",
    "\n",
    "For ordinal $y$, try\n",
    "   * ordinal hinge loss - `OrdinalHingeLoss()`\n",
    "   * bigger vs smaller loss - `BvSLoss()`\n",
    "       * (by default, it uses the logistic loss for the underlying binary classifier)\n",
    "       \n",
    "It also provides a few regularizers, including \n",
    "   * no regularization - `ZeroReg()`\n",
    "   * quadratic regularization - `QuadReg()`\n",
    "   * $\\ell_1$ regularization - `OneReg()`\n",
    "   * nonnegative constraint - `NonNegConstraint()`\n",
    "       \n",
    "Below, we provide some examples for how to use the proxgrad function to fit regularized ERM problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate random data set\n",
    "\n",
    "First (as usual), we'll generate some random data to try our methods on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(0)\n",
    "n = 50\n",
    "d = 10\n",
    "X = randn(n,d)\n",
    "w♮ = randn(d)\n",
    "y = X*w♮ + .1*randn(n);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic loss, quadratic regularizer\n",
    "\n",
    "$$\n",
    "\\mbox{minimize} \\quad \\frac 1 n ||Xw - y||^2 + λ||w||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11180045233465635"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we form \\frac 1 n || ⋅ ||^2 by multiplying the QuadLoss() function by 1/n\n",
    "loss = 1/n*QuadLoss()\n",
    "\n",
    "# we form λ|| ⋅ ||^2 by multiplying the QuadReg() function by λ\n",
    "λ = .1\n",
    "reg = λ*QuadReg()\n",
    "\n",
    "# minimize 1/n ||Xw - y||^2 + λ||w||^2\n",
    "#w = proxgrad(loss, reg, X, y, maxiters=5, c=.1, stepsize=1, max_inner_iters=10000) \n",
    "w = proxgrad(loss, reg, X, y, maxiters=5)\n",
    "\n",
    "norm(X*w-y) / norm(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`maxiters`, the maximum number of iterations, controls how fully we converge.\n",
    "You can try increasing it to see if the error improves.\n",
    "\n",
    "In the next code block, do you think the error will be \n",
    "* A) higher \n",
    "* B) lower\n",
    "* C) the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09941186768969058"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = proxgrad(loss, reg, X, y, maxiters=100) \n",
    "norm(X*w-y) / norm(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge loss, quadratic regularizer\n",
    "\n",
    "$$\n",
    "\\mbox{minimize} \\quad \\frac 1 n \\sum_{i=1}^n (1 - y_i w^T x_i)_+ + λ||w||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ybool = (y.>=0) # form a boolean target\n",
    "\n",
    "# we form \\frac 1 n \\sum_{i=1}^n (1 - ⋅ )_+ by multiplying the HingeLoss() function by 1/n\n",
    "loss = 1/n*HingeLoss()\n",
    "\n",
    "# we form λ|| ⋅ ||^2 by multiplying the QuadReg() function by λ\n",
    "λ = .1\n",
    "reg = λ*QuadReg()\n",
    "\n",
    "# minimize 1/n \\frac 1 n \\sum_{i=1}^n (1 - y_i w^T x_i)_+ + λ||w||^2\n",
    "w = proxgrad(loss, reg, X, ybool, maxiters=10) \n",
    "\n",
    "# predict output values using learned classifier\n",
    "yhat = impute(loss, X*w)\n",
    "\n",
    "# misclassification error \n",
    "(n - sum(yhat .== ybool)) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For nonsmooth problems (like the hinge loss), a smaller stepsize can also help.\n",
    "\n",
    "In the next code block, do you think the error will be\n",
    "\n",
    "* A) higher\n",
    "* B) lower\n",
    "* C) the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = proxgrad(loss, reg, X, ybool, maxiters=100, stepsize=.1) \n",
    "yhat = impute(loss, X*w)\n",
    "\n",
    "# misclassification error \n",
    "(n - sum(yhat .== ybool)) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework question \n",
    "\n",
    "Use the proxgrad function to fit the following objective\n",
    "    \n",
    "$$\n",
    "\\mbox{minimize} \\quad \\frac 1 n \\sum_{i=1}^n \\log(1 + \\exp(- \\text{ybool}_i w^T x_i)) + λ||w||^2\n",
    "$$\n",
    "for $\\lambda = .5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
